\chapter{Performance} % Main chapter title

\label{Chapter3} 


\section{Approach}


Based on the information we gathered in the previous chapter, we will create several proof of concept apps with these technologies.
We will also create a standardized performance benchmark using Cypress. Cypress can run the tests on multiple browsers.


The proof of concept apps must replicate the real behavior as close as possible. 

\begin{itemize}
	\item Load images from the CMS
	\item Perform API requests to a backend
	\item ... ? // TODO: what else does it need to do? Dont go too far with the PoC apps tho
\end{itemize}


The tests will be ran using Docker containers.
// TODO: explain how it works :)
// There's a bunch of moving parts (CMS with database + benchmark + each PoC app)


To accurately gauge performance of each app, we will take into account several factors.

// TODO: Should formalize these and explain what each metric means?

\begin{itemize}
	\item Web vitals  https://web.dev/vitals/
	\item Build time (aka how long does `npm run build` take?)
	\item Lighthouse scores https://web.dev/performance-scoring/
	\item //TODO: Any more?
\end{itemize}

We should also try to run these tests locally vs in a deployed environment. Network latency (or other factors) may play a big role! 

\section{Results}

// TODO: Tables with a bunch of numbers go in the appendix,
// What visualizations can we create to put in here?

// TODO:
Which is fastest? Probably have to weigh different factors together. 
I doubt one technology will be the clear winner here

// TODO:
We should also explain why metrics differ per technology.
For example, I expect static site generators to have a better Largest Contentful Paint than a app that loads CMS assets during runtime